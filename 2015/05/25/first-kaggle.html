<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>My First Kaggle Competition | Heaton Research</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it,Kaggle is an ongoing forum for competitive data science. Individuals and teams compete tocreate the best model">
<meta property="og:type" content="article">
<meta property="og:title" content="My First Kaggle Competition">
<meta property="og:url" content="https://www.heatonresearch.com/2015/05/25/first-kaggle.html">
<meta property="og:site_name" content="Heaton Research">
<meta property="og:description" content="I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it,Kaggle is an ongoing forum for competitive data science. Individuals and teams compete tocreate the best model">
<meta property="og:locale">
<meta property="og:image" content="https://www.heatonresearch.com/images/blog/2015-05-25-first-kaggle-1.jpeg">
<meta property="og:image" content="https://www.heatonresearch.com/images/blog/2015-05-25-first-kaggle-2.png">
<meta property="article:published_time" content="2015-05-25T13:26:00.000Z">
<meta property="article:modified_time" content="2025-01-20T12:40:01.807Z">
<meta property="article:author" content="Jeff Heaton">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.heatonresearch.com/images/blog/2015-05-25-first-kaggle-1.jpeg">
<meta name="twitter:creator" content="@jeffheaton">
  
    <link rel="alternate" href="/atom.xml" title="Heaton Research" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
  

  

  
<link rel="stylesheet" href="/css/styles.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-5393865-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <script data-ad-client="ca-pub-6846576724383320" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
        <a class="navbar-brand" href="/">Heaton Research</a>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/about/">About</a></li>
        
          <li><a class=""
                 href="/jeff_heaton_projects.html">Projects</a></li>
        
          <li><a class=""
                 href="/book/">Books</a></li>
        
          <li><a class=""
                 href="/contact.html">Contact</a></li>
        
          <li><a class=""
                 href="/support.html">Support Me</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <div class="blog-header">
  <h1 class="blog-title">Heaton Research</h1>
  
</div>

<div class="row">
    <div class="col-sm-8 blog-main">
      <article id="post-first-kaggle" class="article article-type-post" itemscope itemprop="blogPost">

  <header class="article-header">
    
  
    <h1 class="article-title" itemprop="name">
      My First Kaggle Competition
    </h1>
  


  </header>

  <div class="article-meta">
    <div class="article-datetime">
  <a href="/2015/05/25/first-kaggle.html" class="article-date"><time datetime="2015-05-25T13:26:00.000Z" itemprop="datePublished">2015-05-25</time></a>
</div>

    
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/kaggle/">kaggle</a>
  </div>


  </div>
  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p>I placed in the top 10% of my first Kaggle competition.  If you are not familiar with it,<br><a target="_blank" rel="noopener" href="https://www.kaggle.com/">Kaggle</a> is an ongoing forum for competitive data science. Individuals and teams compete to<br>create the best model for data sets provided by industry and sometimes academia.<br>Individuals who enter are ranked as either Novice, Kaggler and <a target="_blank" rel="noopener" href="https://www.kaggle.com/wiki/UserRankingAndTierSystem">Kaggle Master</a>.  To become<br>a Kaggle master, one must place in the top 10% of two competitions; and in one of the top<br>10 slots of a third competition.</p>
<p>I’ve talked about Kaggle in many of my presentations.  I’ve also used Kaggle data in<br>my books. Until now, I had yet to actually enter a Kaggle competition.  I decided it was<br>finally time to try this for myself. I competed in the <a href="http://otto%20group%20product%20classification%20challenge/">Otto Group Product Classification</a><br>Challenge that ended on May 18th, 2015.  My score was sufficient to land in the top 10%,<br>so I’ve completed one of the requirements for Kaggle master.  My Kaggle profile can be<br>seen here.</p>
<p>My goals for entering were:</p>
<ul>
<li>See how hard Kaggle actually is, and move towards a Kaggle master designation.</li>
<li>Learn from the other Kagglers and forums.</li>
<li>Build a basic toolkit that I will use for future Kaggle competitions.</li>
<li>Gain an example (from my entry) for the <a href="http://www.heatonresearch.com/aifh">Artificial Intelligence for Humans series</a>.</li>
<li>Maybe get an idea or two for my future dissertation (I am a phd student at <a target="_blank" rel="noopener" href="http://cec.nova.edu/">Nova Southeastern University)</a>.</li>
</ul>
<h2 id="The-Otto-Classification-Challenge"><a href="#The-Otto-Classification-Challenge" class="headerlink" title="The Otto Classification Challenge"></a>The Otto Classification Challenge</h2><p>First, I will give a brief introduction to the exact nature of the Otto Classification<br>Challenge.  For a complete description, refer to the Kaggle description(<a target="_blank" rel="noopener" href="https://www.kaggle.com/c/otto-group-product-classification-challenge">found here</a>).<br>This challenge was introduced by the <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Otto_GmbH">Otto Group</a>, who is the world’s largest mail order<br>company and currently one of the biggest <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/E-commerce">e-commerce</a> companies, mainly based in Germany<br>and France but operating in more than 20 countries.  They have many products sold over<br>numerous countries.  They would like to be able to classify these products into 9<br>categories, using 93 features (columns).  These 93 columns represent counts, and are<br>often zero.</p>
<p>The data are completely redacted.  You do not know what the 9 categories are, nor do you<br>know the meaning behind the 93 features.  You only know that the features are integer<br>counts. Most Kaggle competitions provide you with a test and training dataset.  For the<br>training dataset you are given the outcomes, or correct answers.  For the test set, you<br>are only given the 93 features, and you must provide the outcome.  The test and training<br>sets are divided as follows:</p>
<ul>
<li>Test Data: 144K rows</li>
<li>Training Data: 61K rows</li>
</ul>
<p>You do not actually submit your model to Kaggle.  Rather, you submit your predictions<br>based on the test data.  This allows you to use any platform to make these predictions.<br>The actual format of a submission for this competition is the probability of each of<br>the 9 categories being the outcome.  This is not like a university multiple choice test<br>where you must submit your answer as A, B, C, or D.  Rather, you would submit your<br>answer as:</p>
<ul>
<li>A: 80% probability</li>
<li>B: 16% probability</li>
<li>C: 2% probability</li>
<li>D: 2% probability</li>
</ul>
<p>I wish college exams were graded like this!  Often I am very confident about two of the<br>answers, and can eliminate the other two.  Simply assign a probability to each, and you<br>get a partial score.  If A were the correct answer for the above, I would get 80% of the<br>points.</p>
<p>The actual Kaggle score is slightly more complex than that.  Rather, you are graded on a<br>logarithm based scale and are very heavily penalized for having a lower probability on<br>the correct answer. The following are a few lines from my submission:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1,0.0003,0.2132,0.2340,0.5468,6.2998e-05,0.0001,0.0050,0.0001,4.3826e-05</span><br><span class="line">2,0.0011,0.0029,0.0010,0.0003,0.0001,0.5207,0.0013,0.4711,0.0011</span><br><span class="line">3,3.2977e-06,4.1419e-06,7.4524e-06,2.6550e-06,5.0014e-07,0.9998,5.2621e-06,0.0001,6.6447e-06</span><br><span class="line">4,0.0001,0.6786,0.3162,0.0039,3.3378e-05,4.1196e-05,0.0001,0.0001,0.0006</span><br><span class="line">5,0.1403,0.0002,0.0002,6.734e-05,0.0001,0.0027,0.0009,0.0297,0.8255</span><br></pre></td></tr></table></figure>

<p>Each line starts with a number that specifies the data item that is being answered.<br>The sample above shows the answers for items 1-5.  The next 9 values are the probabilities<br>for each of the product classes.  These probabilities must add up to 1.0 (100%).</p>
<h2 id="What-I-Learned-from-Kaggle"><a href="#What-I-Learned-from-Kaggle" class="headerlink" title="What I Learned from Kaggle"></a>What I Learned from Kaggle</h2><p>If you want to do well in Kaggle, the following are very important topics, along with<br>the tools I used.</p>
<ul>
<li>Deep Learning - Using <a target="_blank" rel="noopener" href="http://h2o.ai/">H2O</a> and <a target="_blank" rel="noopener" href="https://github.com/Lasagne/Lasagne">Lasagne</a></li>
<li>Gradient Boosting Machines (GBM) - <a target="_blank" rel="noopener" href="https://github.com/dmlc/xgboost">Using XGBOOST</a></li>
<li>Ensemble Learning - <a target="_blank" rel="noopener" href="http://www.numpy.org/">Using NumPy</a></li>
<li>Feature Engineering - Using NumPy and <a target="_blank" rel="noopener" href="http://scikit-learn.org/stable/">Scikit-Learn</a></li>
</ul>
<p>The two areas that I learned the most about, during this challenge, were GBM parameter<br>tuning and ensemble learning.  I got pretty good at tuning a GBM.  The individual scores<br>for my GBM’s were in line with those used by the top teams.</p>
<p>Before Kaggle I typically used only one model, if I were using neural networks, I just<br>used neural networks.  If I were using an SVM, Random Forest or Gradient Boosting, I stuck<br>to just that model.  With Kaggle, it is critical to use multiple models, ensembled to<br>produce better results than each of the models could produce independently.</p>
<p>Some of my main takeaways from the competition:</p>
<ul>
<li>GPU is really important for deep learning.  It is best to use a deep learning package that supports it, such as H2O, Theano or Lasagne.</li>
<li>The <a target="_blank" rel="noopener" href="http://lvdmaaten.github.io/tsne/">t-sne</a> visualization is awesome for high-dimension visualization and creating features.</li>
<li>I need to learn to ensemble better!</li>
</ul>
<p>This competition was the first time I used <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">T-SNE</a>.  It works like PCA in that it is capable<br>of reducing dimensions, however, the data points separate in such a way that the<br>visualization is often clearer than <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>. This is done using a stochastic nearest<br>neighbor process. I plan to learn more about how t-sne actually performs the reduction,<br>compared to PCA.</p>
<p><img src="/images/blog/2015-05-25-first-kaggle-1.jpeg" alt="t-SNE Plot of the Otto Group Challenge"></p>
<h2 id="My-Approach-to-the-Otto-Challenge"><a href="#My-Approach-to-the-Otto-Challenge" class="headerlink" title="My Approach to the Otto Challenge"></a>My Approach to the Otto Challenge</h2><p>So far I’ve only worked with single model systems.  I’ve used models that contain ensembles<br>that are “built in”, such as random forests and gradient boosting machines.  However, it<br>is possible to create higher-level ensembles of these models.  I used a total of 20 models,<br>this included 10 deep neural networks and 10 gradient boosting machines.  My deep neural<br>network system provided one prediction and my gradient boosting machines provided the other.<br>These two predictions were blended together, using a simple ratio.  The resulting prediction<br>vector was then normalized so that the sum equaled 1.0(100%).</p>
<p><img src="/images/blog/2015-05-25-first-kaggle-2.png" alt="Jeff Heaton&#39;s Kaggle Model for the Otto Group"></p>
<p>I did not remove or engineer any fields.  For both model types I converted all 93<br>attributes into Z-Scores.  For the neural network I normalized all values to be in a<br>specific range.</p>
<p>My 10 deep learning neural networks used a simple bagging method.  I averaged the<br>predictions from 20 different neural networks.  Each of these neural networks was created<br>by choosing a different 80/20 split between training and validation.  The neural network<br>was trained on the training data until the validation score did not improve for 25 epochs.<br>Once training stopped I used the weights from the epoch that produced the highest training<br>score. This process is a simple form of bagging called <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Bootstrap_aggregating">bootstrap aggregation</a>.</p>
<p>My 10 gradient boosting machines (GBM) were each components of a 10-fold cross-validation.<br>I essentially broke the Kaggle training data into 10 folds and used each of these folds<br>as a validation set, and the others as training.  This produced 10 gradient boosting machines.<br>I then used an NxM coefficient matrix to blend each of these together.  Where N is the<br>number of models, M is the number of features.  In this case it was a 10x9 grid.  This<br>matrix weighted each of the 10 model’s predictive power in each of the 9 categories.<br>These coefficients were a straight probability calculation from the <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a> of<br>each of the 10 models.  This allowed each model to potentially specialize in each of the<br>9 categories.</p>
<p>I spent considerable time tuning my GBM.  I used Nelder-Mead searches to optimize my<br>hyper-parameter vector.  I ultimately settled on the following parameters:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">params = &#123;<span class="string">&#x27;max_depth&#x27;</span>: <span class="number">13</span>,<span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">4</span>,<span class="string">&#x27;subsample&#x27;</span>: <span class="number">.78</span>,<span class="string">&#x27;gamma&#x27;</span>: <span class="number">0</span>,<span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.5</span>, <span class="string">&#x27;eta&#x27;</span>:<span class="number">.005</span>, <span class="string">&#x27;threads&#x27;</span>:<span class="number">24</span>&#125;</span><br><span class="line">Each of these two approaches (GBM <span class="keyword">and</span> neural network) produced a separate submission file. I then blended these together, weighting each.  I found that <span class="number">0.65</span> gave me the best blend <span class="keyword">with</span> my deep neural network.</span><br></pre></td></tr></table></figure>

<h2 id="What-Worked-Well-for-Top-Teams"><a href="#What-Worked-Well-for-Top-Teams" class="headerlink" title="What Worked Well for Top Teams"></a>What Worked Well for Top Teams</h2><p>The top Kaggle teams made use of more sophisticated ensemble techniques than I did.<br>This will be my primary learning area for the next competition.  You can read about<br>some of the top models here:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov">The Top Scoring Model</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14315/strategy-for-top-25-score">Relatively Simple Model for a Top 25 Score</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14297/share-your-models">Share Your Models</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14296/competition-write-up-optimistically-convergent">One of the Top Ten</a></li>
<li><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14295/41599-via-tsne-meta-bagging">TSNE &amp; Meta-Bagging</a></li>
</ul>
<p>The above write-ups are very useful, I’ve already started examining their approaches.</p>
<p>Some of the top technologies discussed were:</p>
<ul>
<li>Feature Engineering</li>
<li>Input Transformation - good write up <a target="_blank" rel="noopener" href="http://fmwww.bc.edu/repec/bocode/t/transint.html">here</a><ul>
<li>log transforms</li>
<li>sqrt(x + 3/8) - Not sure what this one is called, but I saw it used a few times</li>
<li>z-score transforms</li>
<li>ranged transformation</li>
</ul>
</li>
<li>Hyperparameter Optimization<ul>
<li>Nelder-Mead</li>
<li><a target="_blank" rel="noopener" href="https://github.com/JasperSnoek/spearmint">Spearmint</a></li>
</ul>
</li>
</ul>
<p>I will probably not enter another Kaggle until the fall of this year.  This blog post<br>will be updated to contain my notes as I investigate other techniques for this<br>competition.</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="https://www.heatonresearch.com/2015/05/25/first-kaggle.html" data-id="cm659mycc002c98mtdhzr53pc" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      
        <a href="https://www.heatonresearch.com/2015/05/25/first-kaggle.html#disqus_thread" class="article-comment-link">
          <i class="fa fa-comment"></i> Comments
        </a>
      
      

    </footer>
  </div>
  
    
<ul id="article-nav" class="nav nav-pills nav-justified">
  
  <li role="presentation">
    <a href="/2015/03/21/r_big_o.html" id="article-nav-older" class="article-nav-link-wrap">
      <i class="fa fa-chevron-left pull-left"></i>
      <span class="article-nav-link-title">Quick R Tutorial: The Big-O Chart</span>
    </a>
  </li>
  
  
  <li role="presentation">
    <a href="/2015/08/30/madrid-gecco.html" id="article-nav-newer" class="article-nav-link-wrap">
      <span class="article-nav-link-title">Visiting Spain and the GECCO Confrence</span>
      <i class="fa fa-chevron-right pull-right"></i>
    </a>
  </li>
  
</ul>


  
</article>


<section id="comments">
  <div id="disqus_thread">
    <noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
  </div>
</section>


    </div>
    <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
      
  <div class="sidebar-module sidebar-module-inset">
  <h4>About</h4>
  <p>Jeff Heaton, Ph.D. is a YouTuber, [computer/data] [scientist/engineer], and indie publisher. Heaton Research is the homepage for his projects and research.</p>

</div>


  
  <div class="sidebar-module">
    <h4>Categories</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/ai/">ai</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/aifh/">aifh</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/bbs/">bbs</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/course/">course</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/datascience/">datascience</a><span class="sidebar-module-list-count">11</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/encog/">encog</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/gpu/">gpu</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/java/">java</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/kaggle/">kaggle</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/learning/">learning</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/mergelife/">mergelife</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/phd/">phd</a><span class="sidebar-module-list-count">7</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/presentation/">presentation</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/python/">python</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/r/">r</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/tensorflow/">tensorflow</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/categories/wustl/">wustl</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  


  

  
  <div class="sidebar-module">
    <h4>Archives</h4>
    <ul class="sidebar-module-list"><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/09/">September 2019</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/05/">May 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2019/03/">March 2019</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/12/">December 2018</a><span class="sidebar-module-list-count">6</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/11/">November 2018</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/10/">October 2018</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/09/">September 2018</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/08/">August 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2018/01/">January 2018</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/11/">November 2017</a><span class="sidebar-module-list-count">3</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/09/">September 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/08/">August 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/07/">July 2017</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/06/">June 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/05/">May 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/03/">March 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/02/">February 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2017/01/">January 2017</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/09/">September 2016</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/03/">March 2016</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2016/02/">February 2016</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/09/">September 2015</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/08/">August 2015</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/05/">May 2015</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2015/03/">March 2015</a><span class="sidebar-module-list-count">4</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2014/12/">December 2014</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2014/09/">September 2014</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2014/05/">May 2014</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2014/02/">February 2014</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2013/08/">August 2013</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2013/07/">July 2013</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2013/06/">June 2013</a><span class="sidebar-module-list-count">2</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2013/04/">April 2013</a><span class="sidebar-module-list-count">1</span></li><li class="sidebar-module-list-item"><a class="sidebar-module-list-link" href="/archives/2013/03/">March 2013</a><span class="sidebar-module-list-count">1</span></li></ul>
  </div>



  
  <div class="sidebar-module">
    <h4>Recents</h4>
    <ul class="sidebar-module-list">
      
        <li>
          <a href="/2019/09/09/pas-2019-features.html">Session 16: B/I - Multivariate Feature Engineering: Beyond Simple Data Preparation</a>
        </li>
      
        <li>
          <a href="/2019/09/03/tf-no-module-jupyter.html">Why am I getting ImportError: No module named tensorflow?</a>
        </li>
      
        <li>
          <a href="/2019/05/20/2019-video-schedule.html">Video Release Schedule for Fall 2019 Applications of Deep Learning</a>
        </li>
      
        <li>
          <a href="/2019/03/13/tensorflow_20_articles.html">My Favorite TensorFlow 2.0 Articles (as of March 2019)</a>
        </li>
      
        <li>
          <a href="/2018/12/26/youtube-2018-12-26-mergelife-timelapse.html">Time Lapse: Creating Several MergeLife Cellular Automata Online with JavaScript</a>
        </li>
      
    </ul>
  </div>



    </div>
</div>

  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2025 by Heaton Research, Inc. - <a href="/legal/">Legal and Copyright Info</a><br>
Jeff Heaton is a computer scientist, data scientist, and indie publisher. Heaton Research is the homepage for his projects and research. <a href="/tips.html">Tips and support.</a><br><br>
<ul class="list-inline banner-social-buttons">
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://github.com/jeffheaton"><i class="fa fa-github"> <span class="network-name">GitHub</span></i></a></li>
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://twitter.com/jeffheaton"><i class="fa fa-twitter"> <span class="network-name">Twitter</span></i></a></li>
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://www.youtube.com/user/HeatonResearch"><i class="fa fa-youtube-play"> <span class="network-name">Youtube</span></i></a></li>
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://www.facebook.com/encog.framework/"><i class="fa fa-facebook"> <span class="network-name">Facebook</span></i></a></li>
</ul>

    </div>
  </div>
</footer>

  
<script>
  var disqus_shortname = 'heatonresearch';
  
  var disqus_url = 'https://www.heatonresearch.com/2015/05/25/first-kaggle.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>









<script src="/js/script.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>
