<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  
  <title>Techniques for Multi-Threaded Backpropagation for Encog | Heaton Research</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="This was originally posted Mon, 10&#x2F;26&#x2F;2009, and needs some updating.  I will do this soon. This article shows how the Multi Propagation (MPROP) algorithm was implemented for Encog for Java. Though thi">
<meta property="og:type" content="website">
<meta property="og:title" content="Techniques for Multi-Threaded Backpropagation for Encog">
<meta property="og:url" content="https://www.heatonresearch.com/encog/mprop/compare.html">
<meta property="og:site_name" content="Heaton Research">
<meta property="og:description" content="This was originally posted Mon, 10&#x2F;26&#x2F;2009, and needs some updating.  I will do this soon. This article shows how the Multi Propagation (MPROP) algorithm was implemented for Encog for Java. Though thi">
<meta property="og:locale">
<meta property="og:image" content="https://www.heatonresearch.com/images/content/mprop_cores.png">
<meta property="article:published_time" content="2025-01-20T12:40:04.973Z">
<meta property="article:modified_time" content="2025-01-20T12:40:04.973Z">
<meta property="article:author" content="Jeff Heaton">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.heatonresearch.com/images/content/mprop_cores.png">
<meta name="twitter:creator" content="@jeffheaton">
  
    <link rel="alternate" href="/atom.xml" title="Heaton Research" type="application/atom+xml">
  
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">

  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" integrity="sha384-XdYbMnZ/QjLh6iI4ogqCTaIjrFk87ip+ekIjefZch0Y+PvJ8CDYtEs1ipDmPorQ+" crossorigin="anonymous">

  
  

  

  
<link rel="stylesheet" href="/css/styles.css">

  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-5393865-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  <script data-ad-client="ca-pub-6846576724383320" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><nav class="navbar navbar-inverse">
  <div class="container">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#main-menu-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      
        <a class="navbar-brand" href="/">Heaton Research</a>
      
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <div class="collapse navbar-collapse" id="main-menu-navbar">
      <ul class="nav navbar-nav">
        
          <li><a class=""
                 href="/about/">About</a></li>
        
          <li><a class=""
                 href="/jeff_heaton_projects.html">Projects</a></li>
        
          <li><a class=""
                 href="/book/">Books</a></li>
        
          <li><a class=""
                 href="/contact.html">Contact</a></li>
        
          <li><a class=""
                 href="/support.html">Support Me</a></li>
        
          <li><a class=""
                 href="/archives/">Archives</a></li>
        
      </ul>

      <!--
      <ul class="nav navbar-nav navbar-right">
        
          <li><a href="/atom.xml" title="RSS Feed"><i class="fa fa-rss"></i></a></li>
        
      </ul>
      -->
    </div><!-- /.navbar-collapse -->
  </div><!-- /.container-fluid -->
</nav>

  <div class="container">
    <h1>Techniques for Multi-Threaded Backpropagation for Encog</h1>
<div class="row">
    <div class="col-sm-12 blog-main">
      <article id="page-" class="article article-type-page" itemscope itemprop="blogPost">


  <div class="article-inner">

    <div class="article-entry" itemprop="articleBody">
      
        <p><strong>This was originally posted Mon, 10/26/2009, and needs some updating.  I will do this soon.</strong></p>
<p>This article shows how the Multi Propagation (MPROP) algorithm was implemented for Encog for Java. Though this article focuses on the Java implementation the C# version would be very similar. MPROP is based on resilient propagation, but is designed to work well with multicore computers and gain maximum performance.</p>
<p>Multicore computers are becoming more and more common. There seems to be only some many gigahertz that computer manufactures can squeeze out of CPU’s. The real growth in computer performance will likely be in the number of cores contained in a computer’s CPU. As of the writing of this document, October 25, 2009, Intel i7 computers can be had for around $1000 USD. An i7 computer makes use of a Quadcore Hyperthreading CPU. This results in 8 processes being available to programs running on this computer. It is virtually impossible to buy a single-core desktop computer.</p>
<p>Unfortunately programs will not take advantage of these new multicore machines unless the program is written to be multithread. A non-threaded application will simply consume nearly all of the processing power of one core and leave the remaining cores virtually idle. Writing programs to be multithread can be tricky. You must be able to break the task up into smaller packets that each thread can process. At some point the threads usually must communicate with each other and aggregate the job back together.</p>
<p>Neural network training is a very time consuming task. Computers can run for hours, if not days, on a single training task. Supervised neural networks are generally trained with resilient propagation (RPROP) or back propagation. RPROP is the more modern of the two and is almost always the preferred solution. I wanted to enhance the Encog Artificial Intelligence Framework to make use of multithreading to provide fast training on multicore machines. I began Googling for how others might have done it. Unfortunately I did not find much on the topic of multithreaded implementations of back propagation or resilient propagation. I found some solutions, but I had my doubts as to how effective they would with large numbers of processors. I wanted a solution that would work with a potentially large number of processors. I did not want a great deal of synchronization between the threads either, as I may want to run this from a grid of computers at some point.</p>
<p>At this point, as of Encog 2.2, I have a fairly efficient multithreaded training process in place. It only works on a single computer at this point, I will leave a grid implementation for a future version of Encog. This is implemented in the Encog class MultiPropagation. Multi Propagation, or MPROP, is a special training technique introduced in Encog that is based on Resilient Propagation.</p>
<p>A short example is provided that will train a neural network with both MPROP and RPROP. This allows me to compare the overall performance of MPROP on various computer hardware. The following is the main method performs this comparison.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Logging.stopConsoleLogging();</span><br><span class="line">BasicNetwork network = generateNetwork();</span><br><span class="line">NeuralDataSet data = generateTraining();</span><br><span class="line"></span><br><span class="line"><span class="keyword">double</span> rprop = evaluateRPROP(network,data);</span><br><span class="line">network.reset();</span><br><span class="line"><span class="keyword">double</span> mprop = evaluateMPROP(network,data);</span><br><span class="line"><span class="keyword">double</span> factor = rprop/mprop;</span><br><span class="line">System.out.println(<span class="string">&quot;Factor improvement:&quot;</span> + factor);</span><br></pre></td></tr></table></figure>

<p>Both MPROP and RPROP will be fed the same training data and neural network. However, the neural network will be reset for each training algorithm so that no learning from the previous training carries through. The training data is random.</p>
<p>The neural network is composed with the following parameters.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Input Neurons: 40</span><br><span class="line">Output Neurons: 20</span><br><span class="line">Hidden Layer #1 Neurons: 60</span><br></pre></td></tr></table></figure>

<p>The training data is composed of 20,000 input and ideal data pairs.</p>
<p>All of the Encog training algorithms implement the Train interface. This makes them fairly interchangeable. The implementation of the evaluateMPROP and evaluateRPROP is very similar. The implementation of evaluateMPROP is shown here.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">MultiPropagation train = <span class="keyword">new</span> MultiPropagation(network,data);</span><br><span class="line"><span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">System.out.println(<span class="string">&quot;Training 20 Iterations with MPROP&quot;</span>);</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=<span class="number">20</span>;i++)</span><br><span class="line">&#123;</span><br><span class="line">  train.iteration();</span><br><span class="line">  System.out.println(<span class="string">&quot;Iteration #&quot;</span> + i + <span class="string">&quot; Error:&quot;</span> + train.getError());</span><br><span class="line">&#125;</span><br><span class="line">train.finishTraining();</span><br><span class="line"><span class="keyword">long</span> stop = System.currentTimeMillis();</span><br><span class="line"><span class="keyword">double</span> diff = ((<span class="keyword">double</span>)(stop - start))/<span class="number">1000.0</span>;</span><br><span class="line">System.out.println(<span class="string">&quot;MPROP Result:&quot;</span> + diff + <span class="string">&quot; seconds.&quot;</span> );</span><br><span class="line">System.out.println(<span class="string">&quot;Final MPROP error: &quot;</span> + network.calculateError(data));</span><br><span class="line"><span class="keyword">return</span> diff;</span><br></pre></td></tr></table></figure>

<p>This is a typical Encog training routine. Here we loop through 20 training iterations. We track the number of seconds that this took. A similar process is done for RPROP.</p>
<p>Multi Propagation Implementation<br>All propagation training techniques work similarly. Whether it be back propagation, resilient propagation or the Manhattan update rule, the technique is similar. There are two three distinct steps:</p>
<ol>
<li>Perform a Regular Feed Forward Pass</li>
<li>Process the levels backwards and determine the errors at each level</li>
<li>Apply the changes to the weights and thresholds</li>
</ol>
<p>First, a regular feed forward pass is performed. The output from each level is kept so that the error for each level can be evaluated independent. Second, the errors are calculated at each level, and the derivatives of each of the activation functions are used to calculate gradient descents. These gradients will be used in the third step.</p>
<p>The third step is what varies among the different training algorithms. Backpropagation simply takes the gradient descents and scales them by a learning rate. The scaled gradient descents are then directly applied to the weights and thresholds. The Manhattan Update Rule only uses the sign of the gradient to decide in which direction to affect the weight. The weight is then changed in either the positive or negative direction by a fixed constant.</p>
<p>RPROP keeps an individual delta value for every weight and thresholds and only uses the sign of the gradient descent to increase or decrease the delta amounts. The delta amounts are then applied to the weights and thresholds.</p>
<p>The MPROP algorithm uses threads to perform steps 1 &amp; 2. The training data is broken into packets that are distributed among the threads. At the beginning of each iteration threads are started to handle each of these packets. Once all threads have completed, a single thread aggregates all of the results from the threads and applies them to the neural network. There is a very brief amount of time where only one thread is executing, at the end of the iteration. This can be seen from the following monitor.</p>
<p><img src="/images/content/mprop_cores.png" alt="multipropagation (MPROP) on a quadcore"></p>
<p>As you can see from the above image, the i7 is currently running at 100%. You can clearly see the end of each iteration, where each of the processors falls briefly. Fortunately, this is a very brief time and does not have a large impact on overall training efficiency. I did try implementations where I did not force the threads to wait at the end of the iteration for a resynchronization. However these did not provide as efficient of training because the RPROP algorithm, upon which MPROP is based, needs all changes applied before the next iteration begins.</p>
<p>The MPROP algorithms uses a number of threads equal to the number of processors reported by Java, plus 1. Unless there is a single processor, then the MPROP algorithms falls back to a regular single threaded RPROP algorithm.</p>
<h1 id="Testing-Results"><a href="#Testing-Results" class="headerlink" title="Testing Results"></a>Testing Results</h1><p>I tried MPROP, using the above mentioned network and training data on three different computer platforms. First we will look at it on an i7 quadcore.</p>
<p>Dell Studio XPS 8000, 8gig RAM Intel i7 at 2.79GHz</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">Training 20 Iterations with RPROP</span><br><span class="line">Iteration #1 Error:1.0592062021803321</span><br><span class="line">Iteration #2 Error:1.0112968157018771</span><br><span class="line">Iteration #3 Error:0.9650583848127503</span><br><span class="line">Iteration #4 Error:0.9269433225981621</span><br><span class="line">Iteration #5 Error:0.8947162095367102</span><br><span class="line">Iteration #6 Error:0.8714873694194031</span><br><span class="line">Iteration #7 Error:0.8445288449926142</span><br><span class="line">Iteration #8 Error:0.8186688302191717</span><br><span class="line">Iteration #9 Error:0.7952278955734976</span><br><span class="line">Iteration #10 Error:0.7717422560410586</span><br><span class="line">Iteration #11 Error:0.7475048877257578</span><br><span class="line">Iteration #12 Error:0.7235382011165326</span><br><span class="line">Iteration #13 Error:0.7026047081990957</span><br><span class="line">Iteration #14 Error:0.6843757761100023</span><br><span class="line">Iteration #15 Error:0.6685206160475999</span><br><span class="line">Iteration #16 Error:0.6539311876046258</span><br><span class="line">Iteration #17 Error:0.6412660225209257</span><br><span class="line">Iteration #18 Error:0.630790400329957</span><br><span class="line">Iteration #19 Error:0.6211146795350724</span><br><span class="line">Iteration #20 Error:0.6136882493691617</span><br><span class="line">RPROP Result:128.562 seconds.</span><br><span class="line">Final RPROP error: 0.6075224766406004</span><br><span class="line">Training 20 Iterations with MPROP</span><br><span class="line">Iteration #1 Error:0.6075212244066446</span><br><span class="line">Iteration #2 Error:0.8665463281875874</span><br><span class="line">Iteration #3 Error:0.8316846996192032</span><br><span class="line">Iteration #4 Error:0.7451195340393163</span><br><span class="line">Iteration #5 Error:0.7005024644028119</span><br><span class="line">Iteration #6 Error:0.6691870245157884</span><br><span class="line">Iteration #7 Error:0.649034289358449</span><br><span class="line">Iteration #8 Error:0.6339114535879514</span><br><span class="line">Iteration #9 Error:0.6208812103003265</span><br><span class="line">Iteration #10 Error:0.6111566730037973</span><br><span class="line">Iteration #11 Error:0.6056166450414902</span><br><span class="line">Iteration #12 Error:0.6003765685919015</span><br><span class="line">Iteration #13 Error:0.5964873091129251</span><br><span class="line">Iteration #14 Error:0.5932816072550446</span><br><span class="line">Iteration #15 Error:0.5905725872184455</span><br><span class="line">Iteration #16 Error:0.5882703219084173</span><br><span class="line">Iteration #17 Error:0.5863667500894574</span><br><span class="line">Iteration #18 Error:0.5848003831853418</span><br><span class="line">Iteration #19 Error:0.5835759158206529</span><br><span class="line">Iteration #20 Error:0.5823759906353797</span><br><span class="line">MPROP Result:31.88 seconds.</span><br><span class="line">Final MPROP error: 0.5814082684159508</span><br><span class="line">Factor improvement:4.032685069008783</span><br></pre></td></tr></table></figure>


<p>As you can see this machine had a factor improvement of about 4 times. This quadcore uses hyperthreading so 8 processors are reported to Java. Therefore MPROP used 9 threads. Despite the fact that hyperthreading reports twice the number of cores than are physically present, I do not find that it executes anywhere near as fast as additional cores. However, it does help. The fact that I got 4 times with 4 cores is really very good. Threading introduces overhead, without the hyperthreading it is very unlikely that the factor would have been 4 or higher. A factor of 4 implies that the thread switching was perfect, and not even the single threaded synchronization time at the end of each MPROP iteration affected it. Hyperthreading is to thank for this. Still, hyperthreading did not get us anywhere near 7 or 8 times.</p>
<p>Now we will look at a Dual Core computer, with no hyperthreading. Here you see the results from a Dual Core iMac.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">Training 20 Iterations with RPROP</span><br><span class="line">Iteration #1 Error:1.0619945526007815</span><br><span class="line">Iteration #2 Error:1.0173279127855563</span><br><span class="line">Iteration #3 Error:0.9728967012381747</span><br><span class="line">Iteration #4 Error:0.933266210736963</span><br><span class="line">Iteration #5 Error:0.902990819036054</span><br><span class="line">Iteration #6 Error:0.8785929993141319</span><br><span class="line">Iteration #7 Error:0.852770802106324</span><br><span class="line">Iteration #8 Error:0.8297385766666532</span><br><span class="line">Iteration #9 Error:0.8038142080023881</span><br><span class="line">Iteration #10 Error:0.7800412962894463</span><br><span class="line">Iteration #11 Error:0.7587560796078602</span><br><span class="line">Iteration #12 Error:0.7356506865399463</span><br><span class="line">Iteration #13 Error:0.7151090444337569</span><br><span class="line">Iteration #14 Error:0.695744709113637</span><br><span class="line">Iteration #15 Error:0.6788368720802751</span><br><span class="line">Iteration #16 Error:0.6642652711631868</span><br><span class="line">Iteration #17 Error:0.6509332872251975</span><br><span class="line">Iteration #18 Error:0.6397801404435584</span><br><span class="line">Iteration #19 Error:0.630090330257044</span><br><span class="line">Iteration #20 Error:0.6216381426133933</span><br><span class="line">RPROP Result:183.834 seconds.</span><br><span class="line">Final RPROP error: 0.6146150864453944</span><br><span class="line">Training 20 Iterations with MPROP</span><br><span class="line">Iteration #1 Error:0.6146143819685393</span><br><span class="line">Iteration #2 Error:0.861988806667595</span><br><span class="line">Iteration #3 Error:0.8245303438423693</span><br><span class="line">Iteration #4 Error:0.7518132811207181</span><br><span class="line">Iteration #5 Error:0.7081523967374347</span><br><span class="line">Iteration #6 Error:0.6712984917380188</span><br><span class="line">Iteration #7 Error:0.652201535422028</span><br><span class="line">Iteration #8 Error:0.6406601654553405</span><br><span class="line">Iteration #9 Error:0.629090967114433</span><br><span class="line">Iteration #10 Error:0.6175595827587673</span><br><span class="line">Iteration #11 Error:0.6122245715859175</span><br><span class="line">Iteration #12 Error:0.6062311438183174</span><br><span class="line">Iteration #13 Error:0.6013160315144382</span><br><span class="line">Iteration #14 Error:0.5977755359770852</span><br><span class="line">Iteration #15 Error:0.5946842580058522</span><br><span class="line">Iteration #16 Error:0.5920646899231164</span><br><span class="line">Iteration #17 Error:0.5896362050394485</span><br><span class="line">Iteration #18 Error:0.5876920979572654</span><br><span class="line">Iteration #19 Error:0.5859706453734917</span><br><span class="line">Iteration #20 Error:0.5844223947199683</span><br><span class="line">MPROP Result:97.25 seconds.</span><br><span class="line">Final MPROP error: 0.5831411341205304</span><br><span class="line">Factor improvement:1.8903239074550129</span><br></pre></td></tr></table></figure>

<p>As you can see the factor improvement over single threaded RPROP is 1.89. There is no hyperthreading, so it is just the two cores executing. Due to threading overhead and iteration synchronization times, we do not get a perfectly efficient factor of 2.0.</p>
<p>We can also see the results on a single core, hyperthreading computer.</p>
<p>Dimension 8100, Intel Pentium 4 CPU, 3.00 ghtz. 1GB ram</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">Training 20 Iterations with RPROP</span><br><span class="line">Iteration #1 Error:1.055330758837017</span><br><span class="line">Iteration #2 Error:1.0086543528834082</span><br><span class="line">Iteration #3 Error:0.9635498434678869</span><br><span class="line">Iteration #4 Error:0.9234062461127825</span><br><span class="line">Iteration #5 Error:0.893359620995546</span><br><span class="line">Iteration #6 Error:0.8676510392528938</span><br><span class="line">Iteration #7 Error:0.8426219917532086</span><br><span class="line">Iteration #8 Error:0.8176701896241206</span><br><span class="line">Iteration #9 Error:0.7900163103983693</span><br><span class="line">Iteration #10 Error:0.7663833064550073</span><br><span class="line">Iteration #11 Error:0.7423741422166754</span><br><span class="line">Iteration #12 Error:0.7186589370627757</span><br><span class="line">Iteration #13 Error:0.6973331339716679</span><br><span class="line">Iteration #14 Error:0.6786518719968172</span><br><span class="line">Iteration #15 Error:0.6632366051944965</span><br><span class="line">Iteration #16 Error:0.647960280068216</span><br><span class="line">Iteration #17 Error:0.637116419836954</span><br><span class="line">Iteration #18 Error:0.6275179497202836</span><br><span class="line">Iteration #19 Error:0.6193774112511847</span><br><span class="line">Iteration #20 Error:0.6130185860535315</span><br><span class="line">RPROP Result:501.408 seconds.</span><br><span class="line">Final RPROP error: 0.6072752146745306</span><br><span class="line">Training 20 Iterations with MPROP</span><br><span class="line">Iteration #1 Error:0.607274926192379</span><br><span class="line">Iteration #2 Error:0.8534056403923543</span><br><span class="line">Iteration #3 Error:0.8121626914024609</span><br><span class="line">Iteration #4 Error:0.7330129725685066</span><br><span class="line">Iteration #5 Error:0.6952683973977223</span><br><span class="line">Iteration #6 Error:0.6695707142291197</span><br><span class="line">Iteration #7 Error:0.6540724010870805</span><br><span class="line">Iteration #8 Error:0.6338389819166642</span><br><span class="line">Iteration #9 Error:0.6234153712989258</span><br><span class="line">Iteration #10 Error:0.6134344366833728</span><br><span class="line">Iteration #11 Error:0.605518025749937</span><br><span class="line">Iteration #12 Error:0.6015147936004235</span><br><span class="line">Iteration #13 Error:0.5972975056266082</span><br><span class="line">Iteration #14 Error:0.5941906318175534</span><br><span class="line">Iteration #15 Error:0.5912954856379496</span><br><span class="line">Iteration #16 Error:0.5890932691798955</span><br><span class="line">Iteration #17 Error:0.587244744073592</span><br><span class="line">Iteration #18 Error:0.585618048750272</span><br><span class="line">Iteration #19 Error:0.5842085212827279</span><br><span class="line">Iteration #20 Error:0.5829523593081855</span><br><span class="line">MPROP Result:408.015 seconds.</span><br><span class="line">Final MPROP error: 0.5819121474538267</span><br><span class="line">Factor improvement:1.228895996470718</span><br></pre></td></tr></table></figure>

<p>This computer is single core, yet has hyperthreading, so Java reports processors as two. Even with only hyperthreading, a factor improvement is still present, and the MPROP threading algorithm is still beneficial. If MPROP were run on a true single core computer, with no hyperthreading, Java would report the processor count as one and the MPROP algorithms would fall back to single threading.</p>
<h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>MPROP offers great performance improvements over the single threaded MPROP algorithms in cases where a reasonably large training set is present and multicore hardware is used. If neither of these two factors are present, MPROP will fall back to RPROP for training. Because of this, it is the best general purpose training algorithm for Encog.</p>
<p>MPROP was introduced with Encog 2.2, but will continue to be enhanced with future versions of Encog. Improvements will include further optimization of the end of iteration synchronization and moving as much of this synchronization code to the threads as possible. Also options will be added at some point to allow MPROP to operate on a grid of computers.</p>

      
    </div>

    
      

    

    <footer class="article-footer">
      <a data-url="https://www.heatonresearch.com/encog/mprop/compare.html" data-id="cm651nwoc000p8vmt4em17300" class="article-share-link">
        <i class="fa fa-share"></i> Share
      </a>
      

    </footer>
  </div>
  
    

  
</article>

    </div>
</div>

  </div>
  <footer class="blog-footer">
  <div class="container">
    <div id="footer-info" class="inner">
      &copy; 2025 by Heaton Research, Inc. - <a href="/legal/">Legal and Copyright Info</a><br>
Jeff Heaton is a computer scientist, data scientist, and indie publisher. Heaton Research is the homepage for his projects and research. <a href="/tips.html">Tips and support.</a><br><br>
<ul class="list-inline banner-social-buttons">
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://github.com/jeffheaton"><i class="fa fa-github"> <span class="network-name">GitHub</span></i></a></li>
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://twitter.com/jeffheaton"><i class="fa fa-twitter"> <span class="network-name">Twitter</span></i></a></li>
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://www.youtube.com/user/HeatonResearch"><i class="fa fa-youtube-play"> <span class="network-name">Youtube</span></i></a></li>
  <li><a class="btn btn-default btn-sm" target="_blank" rel="noopener" href="https://www.facebook.com/encog.framework/"><i class="fa fa-facebook"> <span class="network-name">Facebook</span></i></a></li>
</ul>

    </div>
  </div>
</footer>

  
<script>
  var disqus_shortname = 'heatonresearch';
  
  var disqus_url = 'https://www.heatonresearch.com/encog/mprop/compare.html';
  
  (function(){
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>


<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha384-8gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>









<script src="/js/script.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>
